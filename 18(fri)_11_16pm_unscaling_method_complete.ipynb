{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM5WhqsN4W4J47HNnMVkV2/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwankang/Deep-Chemometrics/blob/master/18(fri)_11_16pm_unscaling_method_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ONsiak0hQmS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "\n",
        "def get_xY(filename, maxx=600):\n",
        "    \n",
        "    #sio.whosmat(filename)\n",
        "\n",
        "    matcontents = sio.loadmat(filename)\n",
        "    keys = matcontents.keys()\n",
        "    #for key in list(keys):\n",
        "    #    if key[0] == '_':\n",
        "    #        keys.remove(key)\n",
        "            \n",
        "    #keys.sort()\n",
        "            \n",
        "    d = {}            \n",
        "    for key in keys:\n",
        "        data = matcontents[key][0][0]\n",
        "        if key[-1] == \"Y\":\n",
        "            Ydata = data[5]\n",
        "            d[key] = Ydata\n",
        "        else:\n",
        "            xdata = data[5][:,:maxx]\n",
        "            d[key] = xdata\n",
        "            d[\"axisscale\"]= data[7][1][0][0][:maxx].astype(np.float)\n",
        "                    \n",
        "    return d"
      ],
      "metadata": {
        "id": "7FJyx3zwhoi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename='/content/drive/MyDrive/machine_learning/nir_shootout_2003.mat'\n",
        "#dataset = get_xY(filename)\n",
        "#from scipy.io import loadmat\n",
        "#filename = 'Dataset/nir_shootout_2002.mat'\n",
        "matcontents = sio.loadmat(filename)\n",
        "keys = matcontents.keys()\n",
        "list(keys)\n",
        "data=matcontents['test_1'][0][0]\n",
        "X_train=data[7][1][0][0][:600].astype(np.float)\n",
        "#dataset = get_xY(filename)\n",
        "X_train\n",
        "#data1=matcontents['axisscale'][0][0]\n",
        "#axisscale=data1[7][1][0][0][:600].astype(np.float)\n",
        "#axisscale"
      ],
      "metadata": {
        "id": "77It6tm20-6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xdata=data[5][:,:600]\n",
        "xdata"
      ],
      "metadata": {
        "id": "Sa-OdWA7VmNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = matcontents.keys()\n",
        "list(keys)"
      ],
      "metadata": {
        "id": "tQKHT8h9WJB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxx=600\n",
        "d = {}  \n",
        "key2=[ 'calibrate_1',\n",
        " 'calibrate_2',\n",
        " 'test_1',\n",
        " 'test_2',\n",
        " 'validate_1',\n",
        " 'validate_2',\n",
        " 'calibrate_Y',\n",
        " 'test_Y',\n",
        " 'validate_Y']          \n",
        "for key in key2:\n",
        "    data = matcontents[key][0][0]\n",
        "    if key[-1] == \"Y\":\n",
        "        Ydata = data[5]\n",
        "        d[key] = Ydata\n",
        "    else:\n",
        "        xdata = data[5][:,:maxx]\n",
        "        d[key] = xdata\n",
        "        d[\"axisscale\"]= data[7][1][0][0][:maxx].astype(np.float)"
      ],
      "metadata": {
        "id": "lup9Xb3CW4sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d"
      ],
      "metadata": {
        "id": "h9UlKrMtX4G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=d\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#from ChemUtils import GlobalStandardScaler\n",
        "\n",
        "#xscaler = GlobalStandardScaler()\n",
        "X_train = dataset['test_1']\n",
        "X_test = dataset['calibrate_2']\n",
        "#X_train = xscaler.fit_transform(dataset['test_1'])\n",
        "#X_test = xscaler.transform(dataset['calibrate_2'])\n",
        "plt.plot(dataset['axisscale'],X_train.T)\n",
        "plt.show()\n",
        "#print X_train.std()\n",
        "#print X_train.mean()\n",
        "#print X_train.max()\n",
        "#_ = plt.plot(dataset['axisscale'],X_train.T)"
      ],
      "metadata": {
        "id": "jBszPnr8bGXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv1D, Reshape#, MaxPooling1D\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.layers.noise import GaussianNoise"
      ],
      "metadata": {
        "id": "QPimy4KpdOBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters for the network\n",
        "DENSE = 128\n",
        "DROPOUT = 0.5\n",
        "C1_K  = 8 #Number of kernels/feature extractors for first layer\n",
        "C1_S  = 32 #Width of the convolutional mini networks\n",
        "C2_K  = 16\n",
        "C2_S  = 32\n",
        "\n",
        "activation='relu'\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "#The model\n",
        "def make_model():\n",
        "    model = Sequential()\n",
        "    #Adding a bit of GaussianNoise also works as regularization\n",
        "    model.add(GaussianNoise(0.05, input_shape=(input_dim,)))\n",
        "    #First two is number of filter + kernel size\n",
        "    model.add(Reshape((input_dim, 1)))\n",
        "    #model.add(Conv1D(C1_K, (C1_S), activation=activation, border_mode='same'))\n",
        "    #model.add(Conv1D(C2_K, (C2_S), border_mode='same', activation=activation))\n",
        "    model.add(Conv1D(C1_K, (C1_S), activation=activation, padding='same'))\n",
        "    model.add(Conv1D(C2_K, (C2_S), padding='same', activation=activation))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(DROPOUT))\n",
        "    model.add(Dense(DENSE, activation=activation))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    model.compile(loss='mse', optimizer=keras.optimizers.Adadelta(lr=0.01))#, metrics=['mean_absolute_error'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "uBMB_xaSdYfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = make_model()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "e8Iqit-MdReA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_train = yscaler.fit_transform(dataset['test_Y'][:,2])\n",
        "#y_test = yscaler.transform(dataset['calibrate_Y'][:,2]) \n",
        "y_train = dataset['test_Y'][:,2]\n",
        "y_test = dataset['calibrate_Y'][:,2]\n",
        "y_test.size\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "jle-NiVTgGpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#h = model.fit(X_train, y_train, epochs=600, batch_size=8, validation_data=(X_test, y_test), callbacks=[rdlr])\n",
        "h = model.fit(X_train, y_train, epochs=600, batch_size=8, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "YPugoZ6SfuHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ePmTvikIYiRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(h.history['loss'], label='loss')\n",
        "plt.plot(h.history['val_loss'], label='val_loss')\n",
        "\n",
        "#plt.yscale('log')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "ax2 = plt.gca().twinx()\n",
        "ax2.plot(h.history['lr'], color='r')\n",
        "ax2.set_ylabel('lr',color='r')\n",
        "\n",
        "_ = plt.legend()"
      ],
      "metadata": {
        "id": "h1v06nsjmAyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(y_train, model.predict(X_train))\n",
        "plt.scatter(y_test, model.predict(X_test))\n",
        "plt.plot([100,250],[150,250]) # Y = PredY line"
      ],
      "metadata": {
        "id": "jGjpfaiVmmQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_Y_df=model.predict(X_test)\n",
        "Y_df_true=y_test"
      ],
      "metadata": {
        "id": "ifGBPqWQiW-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "mse= mean_squared_error(Y_df_true, pred_Y_df)\n",
        "rmse=np.sqrt(mse)\n",
        "r2=r2_score(Y_df_true, pred_Y_df)\n",
        "print('MSE : {: 0.3f}, || RMSE : {: 0.3f}, || R2 : {: 0.3f}'.format(mse,rmse,r2))"
      ],
      "metadata": {
        "id": "Dlw_PAzfiVyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "end"
      ],
      "metadata": {
        "id": "aBx0JgWgl9iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/EBjerrum/Deep-Chemometrics"
      ],
      "metadata": {
        "id": "-_UTFYpeYcxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ChemUtils"
      ],
      "metadata": {
        "id": "T1ccHcJrYj_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename='/content/drive/MyDrive/machine_learning/nir_shootout_2003.mat'\n",
        "#dataset = get_xY(filename)\n",
        "from scipy.io import loadmat\n",
        "#datafile = \"yourfile.mat\"\n",
        "datafile = filename\n",
        "data = loadmat(datafile, matlab_compatible=True)\n",
        "#data.head()\n",
        "validate_1 = data['validate_1'].squeeze()\n",
        "validate_2 = data['validate_2'].squeeze()\n",
        "validate_Y = data['validate_Y'].squeeze()\n",
        "test_Y = data['test_Y'].squeeze()\n",
        "calibrate_Y = data['calibrate_Y'].squeeze()\n",
        "test_2 = data['test_2'].squeeze()\n",
        "test_1 = data['test_1'].squeeze()\n",
        "calibrate_1= data['calibrate_1'].squeeze()\n",
        "calibrate_2= data['calibrate_2'].squeeze()\n",
        "#axisscale= data['axisscale'].squeeze()\n",
        "#var2 = data['nameOfYourOtherVariable'].squeeze()\n",
        "#[출처] Python을 사용하여 MATLAB 대체: 데이터를 가져오는 방법?|작성자 animepolis185135\n",
        "\n"
      ],
      "metadata": {
        "id": "jqk2WuixhpfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n",
        "import scipy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"\"))\n",
        "\n",
        "from sys import stdout\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.collections as collections\n",
        "import seaborn as sns \n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.optimize import minimize_scalar\n",
        "import scipy.linalg as linalg\n",
        "from cycler import cycler\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler, Binarizer, MinMaxScaler\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict,cross_val_score , KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score, make_scorer, explained_variance_score\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_predict, cross_val_score, KFold , \\\n",
        "    cross_validate, StratifiedKFold"
      ],
      "metadata": {
        "id": "ZlP3cCckuXyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -y tensorflow keras\n",
        "!git clone https://github.com/EBjerrum/Deep-Chemometrics\n",
        "#!git clone https://github.com/EBjerrum"
      ],
      "metadata": {
        "id": "HFtbHEtCu4gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd Deep-Chemometrics\n",
        "!pip install ChemUtils"
      ],
      "metadata": {
        "id": "d7CTqnEnu8ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ChemUtils import GlobalStandardScaler, EmscScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv1D, Reshape#, MaxPooling1D\n",
        "from tensorflow.keras.layers import GaussianNoise\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
        "from livelossplot.tf_keras import PlotLossesCallback\n",
        "# from livelossplot.tf_keras import PlotLossesCallback\n",
        "# from livelossplot.keras import PlotLossesCallback\n",
        "\n",
        "\n",
        "## Define random seeds ir order to maintain reproducible results through multiple testing phases\n",
        "def set_reproducible():\n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "    np.random.seed(12345)\n",
        "    random.seed(12345)\n",
        "    tf.random.set_seed(12345)\n",
        "    \n",
        "set_reproducible()"
      ],
      "metadata": {
        "id": "7OYsvEn9ugqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#from ChemUtils import GlobalStandardScaler\n",
        "\n",
        "#xscaler = GlobalStandardScaler()\n",
        "\n",
        "#X_train = xscaler.fit_transform(dataset['test_1'])\n",
        "#X_train = xscaler.fit_transform(test_1)\n",
        "X_train=test_1\n",
        "#X_test = xscaler.transform(calibrate_2)\n",
        "X_test=calibrate_2\n",
        "#X_test = xscaler.transform(dataset['calibrate_2'])\n",
        "X_train.head()\n",
        "#X_train.std()\n",
        "#print X_train.mean()\n",
        "#print X_train.max()\n",
        "#_ = plt.plot(dataset['axisscale'],X_train.T)"
      ],
      "metadata": {
        "id": "yvfqhkLOtSv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eucDrw_9hs4f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}