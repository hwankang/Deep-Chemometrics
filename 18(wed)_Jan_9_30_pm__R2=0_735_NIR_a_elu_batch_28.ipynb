{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZnllvd1+UiQ8nVHM8an12",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwankang/Deep-Chemometrics/blob/master/18(wed)_Jan_9_30_pm__R2%3D0_735_NIR_a_elu_batch_28.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The program of deep learning with A_NIR Data\n"
      ],
      "metadata": {
        "id": "CI8NLYQFBGRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YiG_eJEo5UNH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v0SnTgFm4nb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "bad54833-821f-4982-c1f0-e72c227e15ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0  ¸ðµ¨ Æ÷ÇÔ    Y_A  899.9280058  900.2520706  900.5763689  \\\n",
              "0              1       True  24.00     0.098681     0.100515     0.102056   \n",
              "1              2       True  24.00     0.098081     0.099926     0.101394   \n",
              "2              3       True  24.00     0.089885     0.092072     0.094055   \n",
              "3              4       True  23.60     0.088701     0.090287     0.091825   \n",
              "4              5       True  23.60     0.095273     0.096945     0.098684   \n",
              "...          ...        ...    ...          ...          ...          ...   \n",
              "2034        2035       True  25.52     0.076506     0.075591     0.074629   \n",
              "2035        2036       True  25.52     0.071916     0.071201     0.070450   \n",
              "2036        2037       True  25.37     0.055557     0.054680     0.053789   \n",
              "2037        2038       True  25.37     0.050590     0.049846     0.049057   \n",
              "2038        2039       True  25.37     0.063837     0.062890     0.062142   \n",
              "\n",
              "      900.9009009  901.2256669  901.5506671  901.8759019  ...  2477.700694  \\\n",
              "0        0.100267     0.096954     0.096161     0.097151  ...     2.968741   \n",
              "1        0.099664     0.096598     0.095915     0.096567  ...     2.960005   \n",
              "2        0.092618     0.089560     0.088900     0.089790  ...     2.943931   \n",
              "3        0.090534     0.087883     0.087348     0.088151  ...     2.935452   \n",
              "4        0.097460     0.094674     0.093949     0.094563  ...     2.966199   \n",
              "...           ...          ...          ...          ...  ...          ...   \n",
              "2034     0.072980     0.072275     0.072979     0.072688  ...     2.962918   \n",
              "2035     0.068871     0.068104     0.068746     0.068455  ...     2.982798   \n",
              "2036     0.052091     0.051338     0.052176     0.052068  ...     2.960384   \n",
              "2037     0.047327     0.046489     0.047292     0.047258  ...     2.968252   \n",
              "2038     0.060671     0.060073     0.060987     0.060884  ...     2.981493   \n",
              "\n",
              "      2480.15873  2482.621648  2485.089463  2487.562189  2490.039841  \\\n",
              "0       2.968988     2.958685     2.950201     2.950122     2.948725   \n",
              "1       2.959948     2.951504     2.945633     2.947553     2.947326   \n",
              "2       2.943679     2.930295     2.919846     2.920668     2.922776   \n",
              "3       2.938259     2.928736     2.921757     2.924166     2.924666   \n",
              "4       2.968021     2.955198     2.942411     2.943006     2.942151   \n",
              "...          ...          ...          ...          ...          ...   \n",
              "2034    2.970997     2.967865     2.957785     2.952054     2.949278   \n",
              "2035    2.982529     2.968697     2.958179     2.962882     2.967330   \n",
              "2036    2.958313     2.946722     2.938467     2.936874     2.936340   \n",
              "2037    2.967836     2.953877     2.942397     2.945923     2.948910   \n",
              "2038    2.981399     2.970350     2.962294     2.963960     2.962348   \n",
              "\n",
              "      2492.522433  2495.00998  2497.502498      2500  \n",
              "0        2.939113    2.929406     2.929555  2.927579  \n",
              "1        2.937901    2.926926     2.924102  2.920722  \n",
              "2        2.913596    2.904503     2.903535  2.901086  \n",
              "3        2.914831    2.905151     2.902643  2.897351  \n",
              "4        2.931640    2.918549     2.918717  2.917849  \n",
              "...           ...         ...          ...       ...  \n",
              "2034     2.941087    2.935639     2.936698  2.938116  \n",
              "2035     2.958188    2.948901     2.948394  2.943151  \n",
              "2036     2.925951    2.920350     2.921682  2.917657  \n",
              "2037     2.940729    2.926084     2.923839  2.919868  \n",
              "2038     2.953168    2.948091     2.949026  2.943110  \n",
              "\n",
              "[2039 rows x 1782 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72f8fd17-6c18-499a-b0f6-0a0b67b47b8e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>¸ðµ¨ Æ÷ÇÔ</th>\n",
              "      <th>Y_A</th>\n",
              "      <th>899.9280058</th>\n",
              "      <th>900.2520706</th>\n",
              "      <th>900.5763689</th>\n",
              "      <th>900.9009009</th>\n",
              "      <th>901.2256669</th>\n",
              "      <th>901.5506671</th>\n",
              "      <th>901.8759019</th>\n",
              "      <th>...</th>\n",
              "      <th>2477.700694</th>\n",
              "      <th>2480.15873</th>\n",
              "      <th>2482.621648</th>\n",
              "      <th>2485.089463</th>\n",
              "      <th>2487.562189</th>\n",
              "      <th>2490.039841</th>\n",
              "      <th>2492.522433</th>\n",
              "      <th>2495.00998</th>\n",
              "      <th>2497.502498</th>\n",
              "      <th>2500</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>24.00</td>\n",
              "      <td>0.098681</td>\n",
              "      <td>0.100515</td>\n",
              "      <td>0.102056</td>\n",
              "      <td>0.100267</td>\n",
              "      <td>0.096954</td>\n",
              "      <td>0.096161</td>\n",
              "      <td>0.097151</td>\n",
              "      <td>...</td>\n",
              "      <td>2.968741</td>\n",
              "      <td>2.968988</td>\n",
              "      <td>2.958685</td>\n",
              "      <td>2.950201</td>\n",
              "      <td>2.950122</td>\n",
              "      <td>2.948725</td>\n",
              "      <td>2.939113</td>\n",
              "      <td>2.929406</td>\n",
              "      <td>2.929555</td>\n",
              "      <td>2.927579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>24.00</td>\n",
              "      <td>0.098081</td>\n",
              "      <td>0.099926</td>\n",
              "      <td>0.101394</td>\n",
              "      <td>0.099664</td>\n",
              "      <td>0.096598</td>\n",
              "      <td>0.095915</td>\n",
              "      <td>0.096567</td>\n",
              "      <td>...</td>\n",
              "      <td>2.960005</td>\n",
              "      <td>2.959948</td>\n",
              "      <td>2.951504</td>\n",
              "      <td>2.945633</td>\n",
              "      <td>2.947553</td>\n",
              "      <td>2.947326</td>\n",
              "      <td>2.937901</td>\n",
              "      <td>2.926926</td>\n",
              "      <td>2.924102</td>\n",
              "      <td>2.920722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "      <td>24.00</td>\n",
              "      <td>0.089885</td>\n",
              "      <td>0.092072</td>\n",
              "      <td>0.094055</td>\n",
              "      <td>0.092618</td>\n",
              "      <td>0.089560</td>\n",
              "      <td>0.088900</td>\n",
              "      <td>0.089790</td>\n",
              "      <td>...</td>\n",
              "      <td>2.943931</td>\n",
              "      <td>2.943679</td>\n",
              "      <td>2.930295</td>\n",
              "      <td>2.919846</td>\n",
              "      <td>2.920668</td>\n",
              "      <td>2.922776</td>\n",
              "      <td>2.913596</td>\n",
              "      <td>2.904503</td>\n",
              "      <td>2.903535</td>\n",
              "      <td>2.901086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>23.60</td>\n",
              "      <td>0.088701</td>\n",
              "      <td>0.090287</td>\n",
              "      <td>0.091825</td>\n",
              "      <td>0.090534</td>\n",
              "      <td>0.087883</td>\n",
              "      <td>0.087348</td>\n",
              "      <td>0.088151</td>\n",
              "      <td>...</td>\n",
              "      <td>2.935452</td>\n",
              "      <td>2.938259</td>\n",
              "      <td>2.928736</td>\n",
              "      <td>2.921757</td>\n",
              "      <td>2.924166</td>\n",
              "      <td>2.924666</td>\n",
              "      <td>2.914831</td>\n",
              "      <td>2.905151</td>\n",
              "      <td>2.902643</td>\n",
              "      <td>2.897351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>23.60</td>\n",
              "      <td>0.095273</td>\n",
              "      <td>0.096945</td>\n",
              "      <td>0.098684</td>\n",
              "      <td>0.097460</td>\n",
              "      <td>0.094674</td>\n",
              "      <td>0.093949</td>\n",
              "      <td>0.094563</td>\n",
              "      <td>...</td>\n",
              "      <td>2.966199</td>\n",
              "      <td>2.968021</td>\n",
              "      <td>2.955198</td>\n",
              "      <td>2.942411</td>\n",
              "      <td>2.943006</td>\n",
              "      <td>2.942151</td>\n",
              "      <td>2.931640</td>\n",
              "      <td>2.918549</td>\n",
              "      <td>2.918717</td>\n",
              "      <td>2.917849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2034</th>\n",
              "      <td>2035</td>\n",
              "      <td>True</td>\n",
              "      <td>25.52</td>\n",
              "      <td>0.076506</td>\n",
              "      <td>0.075591</td>\n",
              "      <td>0.074629</td>\n",
              "      <td>0.072980</td>\n",
              "      <td>0.072275</td>\n",
              "      <td>0.072979</td>\n",
              "      <td>0.072688</td>\n",
              "      <td>...</td>\n",
              "      <td>2.962918</td>\n",
              "      <td>2.970997</td>\n",
              "      <td>2.967865</td>\n",
              "      <td>2.957785</td>\n",
              "      <td>2.952054</td>\n",
              "      <td>2.949278</td>\n",
              "      <td>2.941087</td>\n",
              "      <td>2.935639</td>\n",
              "      <td>2.936698</td>\n",
              "      <td>2.938116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2035</th>\n",
              "      <td>2036</td>\n",
              "      <td>True</td>\n",
              "      <td>25.52</td>\n",
              "      <td>0.071916</td>\n",
              "      <td>0.071201</td>\n",
              "      <td>0.070450</td>\n",
              "      <td>0.068871</td>\n",
              "      <td>0.068104</td>\n",
              "      <td>0.068746</td>\n",
              "      <td>0.068455</td>\n",
              "      <td>...</td>\n",
              "      <td>2.982798</td>\n",
              "      <td>2.982529</td>\n",
              "      <td>2.968697</td>\n",
              "      <td>2.958179</td>\n",
              "      <td>2.962882</td>\n",
              "      <td>2.967330</td>\n",
              "      <td>2.958188</td>\n",
              "      <td>2.948901</td>\n",
              "      <td>2.948394</td>\n",
              "      <td>2.943151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2036</th>\n",
              "      <td>2037</td>\n",
              "      <td>True</td>\n",
              "      <td>25.37</td>\n",
              "      <td>0.055557</td>\n",
              "      <td>0.054680</td>\n",
              "      <td>0.053789</td>\n",
              "      <td>0.052091</td>\n",
              "      <td>0.051338</td>\n",
              "      <td>0.052176</td>\n",
              "      <td>0.052068</td>\n",
              "      <td>...</td>\n",
              "      <td>2.960384</td>\n",
              "      <td>2.958313</td>\n",
              "      <td>2.946722</td>\n",
              "      <td>2.938467</td>\n",
              "      <td>2.936874</td>\n",
              "      <td>2.936340</td>\n",
              "      <td>2.925951</td>\n",
              "      <td>2.920350</td>\n",
              "      <td>2.921682</td>\n",
              "      <td>2.917657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2037</th>\n",
              "      <td>2038</td>\n",
              "      <td>True</td>\n",
              "      <td>25.37</td>\n",
              "      <td>0.050590</td>\n",
              "      <td>0.049846</td>\n",
              "      <td>0.049057</td>\n",
              "      <td>0.047327</td>\n",
              "      <td>0.046489</td>\n",
              "      <td>0.047292</td>\n",
              "      <td>0.047258</td>\n",
              "      <td>...</td>\n",
              "      <td>2.968252</td>\n",
              "      <td>2.967836</td>\n",
              "      <td>2.953877</td>\n",
              "      <td>2.942397</td>\n",
              "      <td>2.945923</td>\n",
              "      <td>2.948910</td>\n",
              "      <td>2.940729</td>\n",
              "      <td>2.926084</td>\n",
              "      <td>2.923839</td>\n",
              "      <td>2.919868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2038</th>\n",
              "      <td>2039</td>\n",
              "      <td>True</td>\n",
              "      <td>25.37</td>\n",
              "      <td>0.063837</td>\n",
              "      <td>0.062890</td>\n",
              "      <td>0.062142</td>\n",
              "      <td>0.060671</td>\n",
              "      <td>0.060073</td>\n",
              "      <td>0.060987</td>\n",
              "      <td>0.060884</td>\n",
              "      <td>...</td>\n",
              "      <td>2.981493</td>\n",
              "      <td>2.981399</td>\n",
              "      <td>2.970350</td>\n",
              "      <td>2.962294</td>\n",
              "      <td>2.963960</td>\n",
              "      <td>2.962348</td>\n",
              "      <td>2.953168</td>\n",
              "      <td>2.948091</td>\n",
              "      <td>2.949026</td>\n",
              "      <td>2.943110</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2039 rows × 1782 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72f8fd17-6c18-499a-b0f6-0a0b67b47b8e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72f8fd17-6c18-499a-b0f6-0a0b67b47b8e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72f8fd17-6c18-499a-b0f6-0a0b67b47b8e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filename_a='/content/drive/MyDrive/machine_learning/A_NIR_DATA_csv.csv'\n",
        "X2_df = pd.read_csv(filename_a,header=0, \n",
        "                   encoding=\"unicode-escape\")\n",
        "X2_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X3_df=X2_df.iloc[:,3:]\n",
        "X3_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ES8kissu5_4z",
        "outputId": "15946d67-22e1-4ff0-cb1f-a41bd64c09ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      899.9280058  900.2520706  900.5763689  900.9009009  901.2256669  \\\n",
              "0        0.098681     0.100515     0.102056     0.100267     0.096954   \n",
              "1        0.098081     0.099926     0.101394     0.099664     0.096598   \n",
              "2        0.089885     0.092072     0.094055     0.092618     0.089560   \n",
              "3        0.088701     0.090287     0.091825     0.090534     0.087883   \n",
              "4        0.095273     0.096945     0.098684     0.097460     0.094674   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "2034     0.076506     0.075591     0.074629     0.072980     0.072275   \n",
              "2035     0.071916     0.071201     0.070450     0.068871     0.068104   \n",
              "2036     0.055557     0.054680     0.053789     0.052091     0.051338   \n",
              "2037     0.050590     0.049846     0.049057     0.047327     0.046489   \n",
              "2038     0.063837     0.062890     0.062142     0.060671     0.060073   \n",
              "\n",
              "      901.5506671  901.8759019  902.2013713  902.5270758  902.8530155  ...  \\\n",
              "0        0.096161     0.097151     0.096790     0.095622     0.095521  ...   \n",
              "1        0.095915     0.096567     0.095609     0.094073     0.093994  ...   \n",
              "2        0.088900     0.089790     0.089088     0.087585     0.087353  ...   \n",
              "3        0.087348     0.088151     0.087593     0.086458     0.086425  ...   \n",
              "4        0.093949     0.094563     0.093743     0.092318     0.092116  ...   \n",
              "...           ...          ...          ...          ...          ...  ...   \n",
              "2034     0.072979     0.072688     0.070171     0.068566     0.069321  ...   \n",
              "2035     0.068746     0.068455     0.065891     0.064076     0.064532  ...   \n",
              "2036     0.052176     0.052068     0.049510     0.047720     0.048518  ...   \n",
              "2037     0.047292     0.047258     0.044905     0.043365     0.044220  ...   \n",
              "2038     0.060987     0.060884     0.058203     0.056162     0.056647  ...   \n",
              "\n",
              "      2477.700694  2480.15873  2482.621648  2485.089463  2487.562189  \\\n",
              "0        2.968741    2.968988     2.958685     2.950201     2.950122   \n",
              "1        2.960005    2.959948     2.951504     2.945633     2.947553   \n",
              "2        2.943931    2.943679     2.930295     2.919846     2.920668   \n",
              "3        2.935452    2.938259     2.928736     2.921757     2.924166   \n",
              "4        2.966199    2.968021     2.955198     2.942411     2.943006   \n",
              "...           ...         ...          ...          ...          ...   \n",
              "2034     2.962918    2.970997     2.967865     2.957785     2.952054   \n",
              "2035     2.982798    2.982529     2.968697     2.958179     2.962882   \n",
              "2036     2.960384    2.958313     2.946722     2.938467     2.936874   \n",
              "2037     2.968252    2.967836     2.953877     2.942397     2.945923   \n",
              "2038     2.981493    2.981399     2.970350     2.962294     2.963960   \n",
              "\n",
              "      2490.039841  2492.522433  2495.00998  2497.502498      2500  \n",
              "0        2.948725     2.939113    2.929406     2.929555  2.927579  \n",
              "1        2.947326     2.937901    2.926926     2.924102  2.920722  \n",
              "2        2.922776     2.913596    2.904503     2.903535  2.901086  \n",
              "3        2.924666     2.914831    2.905151     2.902643  2.897351  \n",
              "4        2.942151     2.931640    2.918549     2.918717  2.917849  \n",
              "...           ...          ...         ...          ...       ...  \n",
              "2034     2.949278     2.941087    2.935639     2.936698  2.938116  \n",
              "2035     2.967330     2.958188    2.948901     2.948394  2.943151  \n",
              "2036     2.936340     2.925951    2.920350     2.921682  2.917657  \n",
              "2037     2.948910     2.940729    2.926084     2.923839  2.919868  \n",
              "2038     2.962348     2.953168    2.948091     2.949026  2.943110  \n",
              "\n",
              "[2039 rows x 1779 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68921f9d-a0d2-4eaf-8a6a-56e3e27eb5e5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>899.9280058</th>\n",
              "      <th>900.2520706</th>\n",
              "      <th>900.5763689</th>\n",
              "      <th>900.9009009</th>\n",
              "      <th>901.2256669</th>\n",
              "      <th>901.5506671</th>\n",
              "      <th>901.8759019</th>\n",
              "      <th>902.2013713</th>\n",
              "      <th>902.5270758</th>\n",
              "      <th>902.8530155</th>\n",
              "      <th>...</th>\n",
              "      <th>2477.700694</th>\n",
              "      <th>2480.15873</th>\n",
              "      <th>2482.621648</th>\n",
              "      <th>2485.089463</th>\n",
              "      <th>2487.562189</th>\n",
              "      <th>2490.039841</th>\n",
              "      <th>2492.522433</th>\n",
              "      <th>2495.00998</th>\n",
              "      <th>2497.502498</th>\n",
              "      <th>2500</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.098681</td>\n",
              "      <td>0.100515</td>\n",
              "      <td>0.102056</td>\n",
              "      <td>0.100267</td>\n",
              "      <td>0.096954</td>\n",
              "      <td>0.096161</td>\n",
              "      <td>0.097151</td>\n",
              "      <td>0.096790</td>\n",
              "      <td>0.095622</td>\n",
              "      <td>0.095521</td>\n",
              "      <td>...</td>\n",
              "      <td>2.968741</td>\n",
              "      <td>2.968988</td>\n",
              "      <td>2.958685</td>\n",
              "      <td>2.950201</td>\n",
              "      <td>2.950122</td>\n",
              "      <td>2.948725</td>\n",
              "      <td>2.939113</td>\n",
              "      <td>2.929406</td>\n",
              "      <td>2.929555</td>\n",
              "      <td>2.927579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.098081</td>\n",
              "      <td>0.099926</td>\n",
              "      <td>0.101394</td>\n",
              "      <td>0.099664</td>\n",
              "      <td>0.096598</td>\n",
              "      <td>0.095915</td>\n",
              "      <td>0.096567</td>\n",
              "      <td>0.095609</td>\n",
              "      <td>0.094073</td>\n",
              "      <td>0.093994</td>\n",
              "      <td>...</td>\n",
              "      <td>2.960005</td>\n",
              "      <td>2.959948</td>\n",
              "      <td>2.951504</td>\n",
              "      <td>2.945633</td>\n",
              "      <td>2.947553</td>\n",
              "      <td>2.947326</td>\n",
              "      <td>2.937901</td>\n",
              "      <td>2.926926</td>\n",
              "      <td>2.924102</td>\n",
              "      <td>2.920722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.089885</td>\n",
              "      <td>0.092072</td>\n",
              "      <td>0.094055</td>\n",
              "      <td>0.092618</td>\n",
              "      <td>0.089560</td>\n",
              "      <td>0.088900</td>\n",
              "      <td>0.089790</td>\n",
              "      <td>0.089088</td>\n",
              "      <td>0.087585</td>\n",
              "      <td>0.087353</td>\n",
              "      <td>...</td>\n",
              "      <td>2.943931</td>\n",
              "      <td>2.943679</td>\n",
              "      <td>2.930295</td>\n",
              "      <td>2.919846</td>\n",
              "      <td>2.920668</td>\n",
              "      <td>2.922776</td>\n",
              "      <td>2.913596</td>\n",
              "      <td>2.904503</td>\n",
              "      <td>2.903535</td>\n",
              "      <td>2.901086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.088701</td>\n",
              "      <td>0.090287</td>\n",
              "      <td>0.091825</td>\n",
              "      <td>0.090534</td>\n",
              "      <td>0.087883</td>\n",
              "      <td>0.087348</td>\n",
              "      <td>0.088151</td>\n",
              "      <td>0.087593</td>\n",
              "      <td>0.086458</td>\n",
              "      <td>0.086425</td>\n",
              "      <td>...</td>\n",
              "      <td>2.935452</td>\n",
              "      <td>2.938259</td>\n",
              "      <td>2.928736</td>\n",
              "      <td>2.921757</td>\n",
              "      <td>2.924166</td>\n",
              "      <td>2.924666</td>\n",
              "      <td>2.914831</td>\n",
              "      <td>2.905151</td>\n",
              "      <td>2.902643</td>\n",
              "      <td>2.897351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.095273</td>\n",
              "      <td>0.096945</td>\n",
              "      <td>0.098684</td>\n",
              "      <td>0.097460</td>\n",
              "      <td>0.094674</td>\n",
              "      <td>0.093949</td>\n",
              "      <td>0.094563</td>\n",
              "      <td>0.093743</td>\n",
              "      <td>0.092318</td>\n",
              "      <td>0.092116</td>\n",
              "      <td>...</td>\n",
              "      <td>2.966199</td>\n",
              "      <td>2.968021</td>\n",
              "      <td>2.955198</td>\n",
              "      <td>2.942411</td>\n",
              "      <td>2.943006</td>\n",
              "      <td>2.942151</td>\n",
              "      <td>2.931640</td>\n",
              "      <td>2.918549</td>\n",
              "      <td>2.918717</td>\n",
              "      <td>2.917849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2034</th>\n",
              "      <td>0.076506</td>\n",
              "      <td>0.075591</td>\n",
              "      <td>0.074629</td>\n",
              "      <td>0.072980</td>\n",
              "      <td>0.072275</td>\n",
              "      <td>0.072979</td>\n",
              "      <td>0.072688</td>\n",
              "      <td>0.070171</td>\n",
              "      <td>0.068566</td>\n",
              "      <td>0.069321</td>\n",
              "      <td>...</td>\n",
              "      <td>2.962918</td>\n",
              "      <td>2.970997</td>\n",
              "      <td>2.967865</td>\n",
              "      <td>2.957785</td>\n",
              "      <td>2.952054</td>\n",
              "      <td>2.949278</td>\n",
              "      <td>2.941087</td>\n",
              "      <td>2.935639</td>\n",
              "      <td>2.936698</td>\n",
              "      <td>2.938116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2035</th>\n",
              "      <td>0.071916</td>\n",
              "      <td>0.071201</td>\n",
              "      <td>0.070450</td>\n",
              "      <td>0.068871</td>\n",
              "      <td>0.068104</td>\n",
              "      <td>0.068746</td>\n",
              "      <td>0.068455</td>\n",
              "      <td>0.065891</td>\n",
              "      <td>0.064076</td>\n",
              "      <td>0.064532</td>\n",
              "      <td>...</td>\n",
              "      <td>2.982798</td>\n",
              "      <td>2.982529</td>\n",
              "      <td>2.968697</td>\n",
              "      <td>2.958179</td>\n",
              "      <td>2.962882</td>\n",
              "      <td>2.967330</td>\n",
              "      <td>2.958188</td>\n",
              "      <td>2.948901</td>\n",
              "      <td>2.948394</td>\n",
              "      <td>2.943151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2036</th>\n",
              "      <td>0.055557</td>\n",
              "      <td>0.054680</td>\n",
              "      <td>0.053789</td>\n",
              "      <td>0.052091</td>\n",
              "      <td>0.051338</td>\n",
              "      <td>0.052176</td>\n",
              "      <td>0.052068</td>\n",
              "      <td>0.049510</td>\n",
              "      <td>0.047720</td>\n",
              "      <td>0.048518</td>\n",
              "      <td>...</td>\n",
              "      <td>2.960384</td>\n",
              "      <td>2.958313</td>\n",
              "      <td>2.946722</td>\n",
              "      <td>2.938467</td>\n",
              "      <td>2.936874</td>\n",
              "      <td>2.936340</td>\n",
              "      <td>2.925951</td>\n",
              "      <td>2.920350</td>\n",
              "      <td>2.921682</td>\n",
              "      <td>2.917657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2037</th>\n",
              "      <td>0.050590</td>\n",
              "      <td>0.049846</td>\n",
              "      <td>0.049057</td>\n",
              "      <td>0.047327</td>\n",
              "      <td>0.046489</td>\n",
              "      <td>0.047292</td>\n",
              "      <td>0.047258</td>\n",
              "      <td>0.044905</td>\n",
              "      <td>0.043365</td>\n",
              "      <td>0.044220</td>\n",
              "      <td>...</td>\n",
              "      <td>2.968252</td>\n",
              "      <td>2.967836</td>\n",
              "      <td>2.953877</td>\n",
              "      <td>2.942397</td>\n",
              "      <td>2.945923</td>\n",
              "      <td>2.948910</td>\n",
              "      <td>2.940729</td>\n",
              "      <td>2.926084</td>\n",
              "      <td>2.923839</td>\n",
              "      <td>2.919868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2038</th>\n",
              "      <td>0.063837</td>\n",
              "      <td>0.062890</td>\n",
              "      <td>0.062142</td>\n",
              "      <td>0.060671</td>\n",
              "      <td>0.060073</td>\n",
              "      <td>0.060987</td>\n",
              "      <td>0.060884</td>\n",
              "      <td>0.058203</td>\n",
              "      <td>0.056162</td>\n",
              "      <td>0.056647</td>\n",
              "      <td>...</td>\n",
              "      <td>2.981493</td>\n",
              "      <td>2.981399</td>\n",
              "      <td>2.970350</td>\n",
              "      <td>2.962294</td>\n",
              "      <td>2.963960</td>\n",
              "      <td>2.962348</td>\n",
              "      <td>2.953168</td>\n",
              "      <td>2.948091</td>\n",
              "      <td>2.949026</td>\n",
              "      <td>2.943110</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2039 rows × 1779 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68921f9d-a0d2-4eaf-8a6a-56e3e27eb5e5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-68921f9d-a0d2-4eaf-8a6a-56e3e27eb5e5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-68921f9d-a0d2-4eaf-8a6a-56e3e27eb5e5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#special run\n",
        "#X_df_1=X3_df.iloc[39:,:]\n",
        "#X_df_2=X3_df.iloc[18:39,:]\n",
        "#X_df_3=X3_df.iloc[0:18,:]\n",
        "#X_df_1=X3_df.iloc[400:,:]\n",
        "#X_df_2=X3_df.iloc[200:400,:]\n",
        "#X_df_3=X3_df.iloc[0:200,:]\n",
        "X_df_1=X3_df.iloc[200:,:]\n",
        "X_df_2=X3_df.iloc[100:200,:]\n",
        "X_df_3=X3_df.iloc[0:100,:]\n",
        "X_train_df=X_df_1\n",
        "X_val_df=X_df_2\n",
        "X_test_df=X_df_3"
      ],
      "metadata": {
        "id": "MLFqdgXh6CMT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y3_df=X2_df.iloc[:,2:3]\n",
        "Y3_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "OFFGOOoz6TTx",
        "outputId": "3b219931-7905-4daa-c1af-d55f862a4e0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Y_A\n",
              "0     24.00\n",
              "1     24.00\n",
              "2     24.00\n",
              "3     23.60\n",
              "4     23.60\n",
              "...     ...\n",
              "2034  25.52\n",
              "2035  25.52\n",
              "2036  25.37\n",
              "2037  25.37\n",
              "2038  25.37\n",
              "\n",
              "[2039 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8d3f4e9-0afd-4f8f-bfd5-911c0c21a5fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y_A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2034</th>\n",
              "      <td>25.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2035</th>\n",
              "      <td>25.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2036</th>\n",
              "      <td>25.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2037</th>\n",
              "      <td>25.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2038</th>\n",
              "      <td>25.37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2039 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8d3f4e9-0afd-4f8f-bfd5-911c0c21a5fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8d3f4e9-0afd-4f8f-bfd5-911c0c21a5fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8d3f4e9-0afd-4f8f-bfd5-911c0c21a5fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Y_df_1=Y3_df.iloc[39:,:]\n",
        "#Y_df_2=Y3_df.iloc[18:39,:]\n",
        "#Y_df_3=Y3_df.iloc[0:18,:]\n",
        "#Y_df_1=Y3_df.iloc[400:,:]\n",
        "#Y_df_2=Y3_df.iloc[200:400,:]\n",
        "#Y_df_3=Y3_df.iloc[0:200,:]\n",
        "Y_df_1=Y3_df.iloc[200:,:]\n",
        "Y_df_2=Y3_df.iloc[100:200,:]\n",
        "Y_df_3=Y3_df.iloc[0:100,:]\n",
        "y_train_df=Y_df_1\n",
        "y_val_df=Y_df_2\n",
        "y_test_df=Y_df_3"
      ],
      "metadata": {
        "id": "k0pgdpfL6sBo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_val_df=X_train_df.append(X_val_df)\n",
        "#X_train_val=X_train_val_df.values\n",
        "X_train_a=X_train_df.values\n",
        "X_val_a=X_val_df.values\n",
        "X_test_a=X_test_df.values\n",
        "y_train=y_train_df.values\n",
        "y_val=y_val_df.values\n",
        "y_test=y_test_df.values"
      ],
      "metadata": {
        "id": "o7gmubNk6-6y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "std = StandardScaler()\n",
        "#X_train_val=X_train_a.append(X_val_a)\n",
        "#std.fit(X_train_val)\n",
        "std.fit(X_train_a)\n",
        "X_train=std.transform(X_train_a)\n",
        "#std.fit(X_val_a)\n",
        "X_val=std.transform(X_val_a)\n",
        "#std.fit(X_test_a)\n",
        "X_test=std.transform(X_test_a)"
      ],
      "metadata": {
        "id": "5MKf_H-n7GQt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(axisscale,X_train.T)\n",
        "plt.xlabel('nm')\n",
        "plt.ylabel('normalized intensity')\n",
        "plt.title('Normalized training data')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "uTg98M1D7I31",
        "outputId": "8abdedb3-b36d-4a41-fe2b-214596bba2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f42d0033db22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxisscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'normalized intensity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'axisscale' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv1D, Reshape#, MaxPooling1D\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.layers.noise import GaussianNoise"
      ],
      "metadata": {
        "id": "Zlk3lnaY64Ev"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters for the network\n",
        "DENSE = 128\n",
        "#DROPOUT = 0.025\n",
        "DROPOUT = 0\n",
        "C1_K  = 8 #Number of kernels/feature extractors for first layer\n",
        "C1_S  = 32 #Width of the convolutional mini networks\n",
        "C2_K  = 16\n",
        "C2_S  = 32\n",
        "\n",
        "#activation='relu'\n",
        "activation='elu'\n",
        "#activation='sigmoid'\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "#The model\n",
        "def make_model():\n",
        "    model = Sequential()\n",
        "    #Adding a bit of GaussianNoise also works as regularization\n",
        "    #model.add(GaussianNoise(0.05, input_shape=(input_dim,)))\n",
        "    #model.add(GaussianNoise(0.005, input_shape=(input_dim,)))\n",
        "    model.add(GaussianNoise(0.004, input_shape=(input_dim,)))\n",
        "    #First two is number of filter + kernel size\n",
        "    model.add(Reshape((input_dim, 1)))\n",
        "    #model.add(Conv1D(C1_K, (C1_S), activation=activation, border_mode='same'))\n",
        "    #model.add(Conv1D(C2_K, (C2_S), border_mode='same', activation=activation))\n",
        "    model.add(Conv1D(C1_K, (C1_S), activation=activation, padding='same'))\n",
        "    model.add(Conv1D(C2_K, (C2_S), padding='same', activation=activation))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(DROPOUT))\n",
        "    model.add(Dense(DENSE, activation=activation))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    #model.compile(loss='mse', optimizer=keras.optimizers.Adadelta(lr=0.01))#, metrics=['mean_absolute_error'])\n",
        "    model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=0.002))#, metrics=['mean_absolute_error'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "YwcPujsz9iKr"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = make_model()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R0FnsQT98rw",
        "outputId": "d57c2245-5f91-4159-cc7f-5a69202b56dc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gaussian_noise_5 (GaussianN  (None, 1779)             0         \n",
            " oise)                                                           \n",
            "                                                                 \n",
            " reshape_5 (Reshape)         (None, 1779, 1)           0         \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 1779, 8)           264       \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 1779, 16)          4112      \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 28464)             0         \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 28464)             0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 128)               3643520   \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,648,025\n",
            "Trainable params: 3,648,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h = model.fit(X_train, y_train, epochs=400, batch_size=28, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_loUMTe-Bhs",
        "outputId": "c091182e-2994-4354-fe9a-a986d22439c7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "66/66 [==============================] - 1s 7ms/step - loss: 2.8239 - val_loss: 9.0024\n",
            "Epoch 2/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5170 - val_loss: 10.2260\n",
            "Epoch 3/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.5753 - val_loss: 8.9602\n",
            "Epoch 4/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5324 - val_loss: 8.8741\n",
            "Epoch 5/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.6301 - val_loss: 11.5462\n",
            "Epoch 6/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.6749 - val_loss: 8.6280\n",
            "Epoch 7/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.6810 - val_loss: 8.6881\n",
            "Epoch 8/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.4849 - val_loss: 9.4424\n",
            "Epoch 9/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.4095 - val_loss: 8.9110\n",
            "Epoch 10/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.4361 - val_loss: 8.8869\n",
            "Epoch 11/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5041 - val_loss: 10.5996\n",
            "Epoch 12/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5064 - val_loss: 8.4253\n",
            "Epoch 13/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.4114 - val_loss: 8.1892\n",
            "Epoch 14/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3320 - val_loss: 9.4249\n",
            "Epoch 15/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3607 - val_loss: 9.2976\n",
            "Epoch 16/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.6677 - val_loss: 8.6283\n",
            "Epoch 17/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.7322 - val_loss: 8.6834\n",
            "Epoch 18/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5622 - val_loss: 8.6103\n",
            "Epoch 19/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.8456 - val_loss: 9.7967\n",
            "Epoch 20/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.4522 - val_loss: 8.8902\n",
            "Epoch 21/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5333 - val_loss: 8.8050\n",
            "Epoch 22/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.5136 - val_loss: 9.7188\n",
            "Epoch 23/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.5681 - val_loss: 10.1209\n",
            "Epoch 24/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.6804 - val_loss: 8.6810\n",
            "Epoch 25/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3203 - val_loss: 9.1911\n",
            "Epoch 26/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 3.1981 - val_loss: 9.9307\n",
            "Epoch 27/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 4.9856 - val_loss: 28.5680\n",
            "Epoch 28/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 6.9759 - val_loss: 8.8666\n",
            "Epoch 29/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 3.9660 - val_loss: 10.6245\n",
            "Epoch 30/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 3.7423 - val_loss: 11.1978\n",
            "Epoch 31/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 3.1933 - val_loss: 9.0276\n",
            "Epoch 32/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.9573 - val_loss: 9.0480\n",
            "Epoch 33/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.7967 - val_loss: 9.0066\n",
            "Epoch 34/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.7047 - val_loss: 8.5683\n",
            "Epoch 35/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.9546 - val_loss: 8.7804\n",
            "Epoch 36/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.8657 - val_loss: 8.1153\n",
            "Epoch 37/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.6179 - val_loss: 8.2319\n",
            "Epoch 38/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.6428 - val_loss: 8.2662\n",
            "Epoch 39/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.8009 - val_loss: 8.2191\n",
            "Epoch 40/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.8763 - val_loss: 11.4385\n",
            "Epoch 41/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5124 - val_loss: 8.7454\n",
            "Epoch 42/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.6499 - val_loss: 10.0242\n",
            "Epoch 43/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5728 - val_loss: 8.9479\n",
            "Epoch 44/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3174 - val_loss: 8.0849\n",
            "Epoch 45/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.6806 - val_loss: 9.0354\n",
            "Epoch 46/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.7657 - val_loss: 8.5111\n",
            "Epoch 47/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.3599 - val_loss: 9.1424\n",
            "Epoch 48/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3015 - val_loss: 10.2587\n",
            "Epoch 49/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.5075 - val_loss: 9.8815\n",
            "Epoch 50/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.6165 - val_loss: 8.8062\n",
            "Epoch 51/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5904 - val_loss: 9.0415\n",
            "Epoch 52/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.6113 - val_loss: 7.4957\n",
            "Epoch 53/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.3691 - val_loss: 9.1805\n",
            "Epoch 54/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.4319 - val_loss: 10.5960\n",
            "Epoch 55/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3967 - val_loss: 8.8279\n",
            "Epoch 56/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.6951 - val_loss: 8.8807\n",
            "Epoch 57/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3781 - val_loss: 9.3979\n",
            "Epoch 58/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5183 - val_loss: 8.3409\n",
            "Epoch 59/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5082 - val_loss: 7.8839\n",
            "Epoch 60/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5918 - val_loss: 7.9842\n",
            "Epoch 61/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.2754 - val_loss: 8.9817\n",
            "Epoch 62/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3900 - val_loss: 9.7028\n",
            "Epoch 63/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3144 - val_loss: 10.8155\n",
            "Epoch 64/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3754 - val_loss: 8.8629\n",
            "Epoch 65/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.1129 - val_loss: 8.2307\n",
            "Epoch 66/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.1509 - val_loss: 8.9844\n",
            "Epoch 67/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.0960 - val_loss: 7.7637\n",
            "Epoch 68/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.2167 - val_loss: 8.4610\n",
            "Epoch 69/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3434 - val_loss: 8.5368\n",
            "Epoch 70/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3056 - val_loss: 7.5358\n",
            "Epoch 71/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.2438 - val_loss: 8.1542\n",
            "Epoch 72/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.1432 - val_loss: 10.0516\n",
            "Epoch 73/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.4296 - val_loss: 7.9170\n",
            "Epoch 74/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.4153 - val_loss: 10.3281\n",
            "Epoch 75/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.3428 - val_loss: 7.6826\n",
            "Epoch 76/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.1694 - val_loss: 8.6379\n",
            "Epoch 77/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.4052 - val_loss: 8.1416\n",
            "Epoch 78/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.8180 - val_loss: 8.0734\n",
            "Epoch 79/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.6331 - val_loss: 10.5424\n",
            "Epoch 80/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3439 - val_loss: 7.7923\n",
            "Epoch 81/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0422 - val_loss: 8.9022\n",
            "Epoch 82/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0345 - val_loss: 8.5684\n",
            "Epoch 83/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.0269 - val_loss: 8.6753\n",
            "Epoch 84/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0492 - val_loss: 9.1284\n",
            "Epoch 85/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.1531 - val_loss: 8.3396\n",
            "Epoch 86/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.1288 - val_loss: 9.9534\n",
            "Epoch 87/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.2336 - val_loss: 9.9138\n",
            "Epoch 88/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0925 - val_loss: 7.7208\n",
            "Epoch 89/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.1997 - val_loss: 8.4751\n",
            "Epoch 90/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.5242 - val_loss: 8.0925\n",
            "Epoch 91/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.4096 - val_loss: 8.1347\n",
            "Epoch 92/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.0721 - val_loss: 9.5825\n",
            "Epoch 93/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0055 - val_loss: 7.9836\n",
            "Epoch 94/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.9965 - val_loss: 8.5497\n",
            "Epoch 95/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0425 - val_loss: 8.4309\n",
            "Epoch 96/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.1329 - val_loss: 8.6159\n",
            "Epoch 97/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0247 - val_loss: 7.7128\n",
            "Epoch 98/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0835 - val_loss: 8.7427\n",
            "Epoch 99/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.0506 - val_loss: 7.8885\n",
            "Epoch 100/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.1187 - val_loss: 8.3695\n",
            "Epoch 101/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0880 - val_loss: 8.3548\n",
            "Epoch 102/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.0376 - val_loss: 7.7033\n",
            "Epoch 103/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.9785 - val_loss: 8.7650\n",
            "Epoch 104/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.1478 - val_loss: 10.2545\n",
            "Epoch 105/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.2316 - val_loss: 7.3344\n",
            "Epoch 106/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.1445 - val_loss: 7.5094\n",
            "Epoch 107/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0530 - val_loss: 8.6029\n",
            "Epoch 108/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.8882 - val_loss: 8.5149\n",
            "Epoch 109/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.1381 - val_loss: 7.2723\n",
            "Epoch 110/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.1270 - val_loss: 7.3440\n",
            "Epoch 111/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3412 - val_loss: 10.3505\n",
            "Epoch 112/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.2255 - val_loss: 8.4248\n",
            "Epoch 113/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.8844 - val_loss: 8.0186\n",
            "Epoch 114/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.9017 - val_loss: 8.0590\n",
            "Epoch 115/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.1020 - val_loss: 8.5922\n",
            "Epoch 116/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.3539 - val_loss: 7.4003\n",
            "Epoch 117/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.8444 - val_loss: 7.7683\n",
            "Epoch 118/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.1258 - val_loss: 9.5854\n",
            "Epoch 119/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.9715 - val_loss: 8.6830\n",
            "Epoch 120/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0681 - val_loss: 7.8382\n",
            "Epoch 121/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0513 - val_loss: 8.7268\n",
            "Epoch 122/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.8778 - val_loss: 8.2028\n",
            "Epoch 123/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.9344 - val_loss: 7.8568\n",
            "Epoch 124/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.7455 - val_loss: 8.8740\n",
            "Epoch 125/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.8458 - val_loss: 8.5360\n",
            "Epoch 126/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.9591 - val_loss: 8.8622\n",
            "Epoch 127/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.9787 - val_loss: 8.7131\n",
            "Epoch 128/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.8996 - val_loss: 8.0052\n",
            "Epoch 129/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.7887 - val_loss: 7.1598\n",
            "Epoch 130/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.0100 - val_loss: 10.6320\n",
            "Epoch 131/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.8491 - val_loss: 7.5829\n",
            "Epoch 132/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.6622 - val_loss: 8.0127\n",
            "Epoch 133/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.8466 - val_loss: 8.7957\n",
            "Epoch 134/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.7509 - val_loss: 7.5890\n",
            "Epoch 135/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6640 - val_loss: 8.0324\n",
            "Epoch 136/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.7623 - val_loss: 8.1691\n",
            "Epoch 137/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.7781 - val_loss: 9.6541\n",
            "Epoch 138/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.9068 - val_loss: 8.0942\n",
            "Epoch 139/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.7991 - val_loss: 8.5568\n",
            "Epoch 140/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.7290 - val_loss: 7.7835\n",
            "Epoch 141/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6988 - val_loss: 8.0946\n",
            "Epoch 142/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6583 - val_loss: 8.3416\n",
            "Epoch 143/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.8206 - val_loss: 9.0623\n",
            "Epoch 144/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.2788 - val_loss: 10.8294\n",
            "Epoch 145/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.7177 - val_loss: 8.7346\n",
            "Epoch 146/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.7627 - val_loss: 10.7554\n",
            "Epoch 147/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.7898 - val_loss: 8.4497\n",
            "Epoch 148/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.8773 - val_loss: 9.4356\n",
            "Epoch 149/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.8166 - val_loss: 7.8721\n",
            "Epoch 150/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.7701 - val_loss: 8.3427\n",
            "Epoch 151/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6608 - val_loss: 8.3866\n",
            "Epoch 152/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 2.2621 - val_loss: 6.8642\n",
            "Epoch 153/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.8467 - val_loss: 7.2945\n",
            "Epoch 154/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.6393 - val_loss: 8.1740\n",
            "Epoch 155/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.7424 - val_loss: 7.6191\n",
            "Epoch 156/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6707 - val_loss: 8.1171\n",
            "Epoch 157/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.8811 - val_loss: 8.1579\n",
            "Epoch 158/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6152 - val_loss: 8.9990\n",
            "Epoch 159/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6687 - val_loss: 7.7836\n",
            "Epoch 160/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.9077 - val_loss: 9.7520\n",
            "Epoch 161/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.9802 - val_loss: 7.8127\n",
            "Epoch 162/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.7200 - val_loss: 7.9902\n",
            "Epoch 163/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6406 - val_loss: 9.1256\n",
            "Epoch 164/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 2.5660 - val_loss: 9.3806\n",
            "Epoch 165/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.9296 - val_loss: 7.9866\n",
            "Epoch 166/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6951 - val_loss: 8.2013\n",
            "Epoch 167/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6657 - val_loss: 7.7786\n",
            "Epoch 168/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5467 - val_loss: 8.1849\n",
            "Epoch 169/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.5191 - val_loss: 8.0788\n",
            "Epoch 170/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.5707 - val_loss: 9.2745\n",
            "Epoch 171/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6319 - val_loss: 7.8362\n",
            "Epoch 172/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5695 - val_loss: 7.6190\n",
            "Epoch 173/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6403 - val_loss: 9.4478\n",
            "Epoch 174/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6749 - val_loss: 8.8966\n",
            "Epoch 175/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.7271 - val_loss: 7.4329\n",
            "Epoch 176/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5093 - val_loss: 8.7079\n",
            "Epoch 177/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.9744 - val_loss: 8.1880\n",
            "Epoch 178/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5871 - val_loss: 11.3488\n",
            "Epoch 179/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.7097 - val_loss: 6.8853\n",
            "Epoch 180/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.5206 - val_loss: 9.2894\n",
            "Epoch 181/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6904 - val_loss: 9.5736\n",
            "Epoch 182/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5728 - val_loss: 7.2166\n",
            "Epoch 183/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.5572 - val_loss: 9.3551\n",
            "Epoch 184/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4146 - val_loss: 8.4172\n",
            "Epoch 185/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3836 - val_loss: 8.9676\n",
            "Epoch 186/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.8823 - val_loss: 7.7520\n",
            "Epoch 187/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5312 - val_loss: 8.9356\n",
            "Epoch 188/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4561 - val_loss: 8.1187\n",
            "Epoch 189/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5212 - val_loss: 7.9407\n",
            "Epoch 190/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.6675 - val_loss: 8.4850\n",
            "Epoch 191/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6531 - val_loss: 8.0077\n",
            "Epoch 192/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5292 - val_loss: 7.4988\n",
            "Epoch 193/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4687 - val_loss: 9.0651\n",
            "Epoch 194/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6216 - val_loss: 7.7240\n",
            "Epoch 195/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6354 - val_loss: 7.7836\n",
            "Epoch 196/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.7548 - val_loss: 8.0997\n",
            "Epoch 197/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4569 - val_loss: 9.7323\n",
            "Epoch 198/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4245 - val_loss: 8.5279\n",
            "Epoch 199/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5948 - val_loss: 8.6992\n",
            "Epoch 200/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5384 - val_loss: 9.2795\n",
            "Epoch 201/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.3941 - val_loss: 9.0594\n",
            "Epoch 202/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5159 - val_loss: 8.0948\n",
            "Epoch 203/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3458 - val_loss: 8.5572\n",
            "Epoch 204/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3107 - val_loss: 8.1551\n",
            "Epoch 205/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.3475 - val_loss: 9.3265\n",
            "Epoch 206/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.5216 - val_loss: 9.8270\n",
            "Epoch 207/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5872 - val_loss: 8.7460\n",
            "Epoch 208/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5825 - val_loss: 7.6751\n",
            "Epoch 209/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5084 - val_loss: 7.9294\n",
            "Epoch 210/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4822 - val_loss: 8.2944\n",
            "Epoch 211/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.5410 - val_loss: 8.2741\n",
            "Epoch 212/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4342 - val_loss: 7.3444\n",
            "Epoch 213/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3771 - val_loss: 8.4871\n",
            "Epoch 214/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4453 - val_loss: 7.5698\n",
            "Epoch 215/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4144 - val_loss: 9.1361\n",
            "Epoch 216/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3303 - val_loss: 8.7709\n",
            "Epoch 217/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4076 - val_loss: 8.7083\n",
            "Epoch 218/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4346 - val_loss: 8.0406\n",
            "Epoch 219/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5382 - val_loss: 7.9458\n",
            "Epoch 220/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4454 - val_loss: 8.2505\n",
            "Epoch 221/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2292 - val_loss: 8.0472\n",
            "Epoch 222/400\n",
            "66/66 [==============================] - 0s 4ms/step - loss: 1.2691 - val_loss: 8.8795\n",
            "Epoch 223/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3121 - val_loss: 8.3845\n",
            "Epoch 224/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6449 - val_loss: 10.0097\n",
            "Epoch 225/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.6579 - val_loss: 9.5398\n",
            "Epoch 226/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2731 - val_loss: 9.0698\n",
            "Epoch 227/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2441 - val_loss: 8.3251\n",
            "Epoch 228/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3484 - val_loss: 11.0577\n",
            "Epoch 229/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2383 - val_loss: 8.1973\n",
            "Epoch 230/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4057 - val_loss: 9.6943\n",
            "Epoch 231/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3054 - val_loss: 8.4247\n",
            "Epoch 232/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2093 - val_loss: 9.3963\n",
            "Epoch 233/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1971 - val_loss: 9.7713\n",
            "Epoch 234/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1981 - val_loss: 8.3530\n",
            "Epoch 235/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3133 - val_loss: 8.2843\n",
            "Epoch 236/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.5214 - val_loss: 9.4483\n",
            "Epoch 237/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4813 - val_loss: 8.3797\n",
            "Epoch 238/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3901 - val_loss: 10.2098\n",
            "Epoch 239/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2983 - val_loss: 8.4047\n",
            "Epoch 240/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1628 - val_loss: 8.4965\n",
            "Epoch 241/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2049 - val_loss: 8.6869\n",
            "Epoch 242/400\n",
            "66/66 [==============================] - 0s 7ms/step - loss: 1.2220 - val_loss: 8.4215\n",
            "Epoch 243/400\n",
            "66/66 [==============================] - 0s 6ms/step - loss: 1.5381 - val_loss: 8.8715\n",
            "Epoch 244/400\n",
            "66/66 [==============================] - 0s 6ms/step - loss: 1.2361 - val_loss: 9.3988\n",
            "Epoch 245/400\n",
            "66/66 [==============================] - 0s 6ms/step - loss: 1.1663 - val_loss: 8.4876\n",
            "Epoch 246/400\n",
            "66/66 [==============================] - 0s 6ms/step - loss: 1.3789 - val_loss: 9.3682\n",
            "Epoch 247/400\n",
            "66/66 [==============================] - 0s 6ms/step - loss: 1.4431 - val_loss: 8.2137\n",
            "Epoch 248/400\n",
            "66/66 [==============================] - 0s 6ms/step - loss: 1.1437 - val_loss: 9.0206\n",
            "Epoch 249/400\n",
            "66/66 [==============================] - 0s 6ms/step - loss: 1.3870 - val_loss: 7.8351\n",
            "Epoch 250/400\n",
            "66/66 [==============================] - 0s 6ms/step - loss: 1.3199 - val_loss: 7.6239\n",
            "Epoch 251/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1437 - val_loss: 9.0858\n",
            "Epoch 252/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1518 - val_loss: 9.2325\n",
            "Epoch 253/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1716 - val_loss: 8.4606\n",
            "Epoch 254/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1739 - val_loss: 9.0305\n",
            "Epoch 255/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0854 - val_loss: 9.2442\n",
            "Epoch 256/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3537 - val_loss: 8.3277\n",
            "Epoch 257/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1591 - val_loss: 9.1680\n",
            "Epoch 258/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2319 - val_loss: 8.2588\n",
            "Epoch 259/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2387 - val_loss: 9.3315\n",
            "Epoch 260/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2066 - val_loss: 9.0912\n",
            "Epoch 261/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1861 - val_loss: 8.8100\n",
            "Epoch 262/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1375 - val_loss: 7.9000\n",
            "Epoch 263/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2597 - val_loss: 10.1252\n",
            "Epoch 264/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2790 - val_loss: 8.1999\n",
            "Epoch 265/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3177 - val_loss: 8.0550\n",
            "Epoch 266/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1092 - val_loss: 8.3625\n",
            "Epoch 267/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.4628 - val_loss: 8.5888\n",
            "Epoch 268/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2194 - val_loss: 9.3518\n",
            "Epoch 269/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3235 - val_loss: 7.9796\n",
            "Epoch 270/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2059 - val_loss: 8.4549\n",
            "Epoch 271/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0307 - val_loss: 9.7941\n",
            "Epoch 272/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0537 - val_loss: 7.7428\n",
            "Epoch 273/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0742 - val_loss: 8.5458\n",
            "Epoch 274/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1790 - val_loss: 7.7136\n",
            "Epoch 275/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0335 - val_loss: 8.0438\n",
            "Epoch 276/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1726 - val_loss: 8.4775\n",
            "Epoch 277/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0324 - val_loss: 8.0371\n",
            "Epoch 278/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0270 - val_loss: 8.3502\n",
            "Epoch 279/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9913 - val_loss: 7.6498\n",
            "Epoch 280/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.2689 - val_loss: 8.5636\n",
            "Epoch 281/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3926 - val_loss: 9.7372\n",
            "Epoch 282/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0987 - val_loss: 9.2668\n",
            "Epoch 283/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3280 - val_loss: 8.5319\n",
            "Epoch 284/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9673 - val_loss: 9.4361\n",
            "Epoch 285/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.3897 - val_loss: 9.0209\n",
            "Epoch 286/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1625 - val_loss: 8.4885\n",
            "Epoch 287/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0455 - val_loss: 8.5042\n",
            "Epoch 288/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1150 - val_loss: 8.4263\n",
            "Epoch 289/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0914 - val_loss: 10.9874\n",
            "Epoch 290/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0389 - val_loss: 8.8437\n",
            "Epoch 291/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1381 - val_loss: 8.4756\n",
            "Epoch 292/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9559 - val_loss: 8.6475\n",
            "Epoch 293/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9194 - val_loss: 9.3039\n",
            "Epoch 294/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9379 - val_loss: 9.5846\n",
            "Epoch 295/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9762 - val_loss: 8.4558\n",
            "Epoch 296/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9261 - val_loss: 8.8644\n",
            "Epoch 297/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0365 - val_loss: 8.4815\n",
            "Epoch 298/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1490 - val_loss: 9.0623\n",
            "Epoch 299/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1915 - val_loss: 8.4589\n",
            "Epoch 300/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0880 - val_loss: 8.7065\n",
            "Epoch 301/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9434 - val_loss: 8.8917\n",
            "Epoch 302/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0520 - val_loss: 9.2008\n",
            "Epoch 303/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0257 - val_loss: 7.9153\n",
            "Epoch 304/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9574 - val_loss: 8.9724\n",
            "Epoch 305/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0378 - val_loss: 9.4473\n",
            "Epoch 306/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9128 - val_loss: 8.9201\n",
            "Epoch 307/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0748 - val_loss: 8.9513\n",
            "Epoch 308/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9492 - val_loss: 9.8006\n",
            "Epoch 309/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0438 - val_loss: 7.9778\n",
            "Epoch 310/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8948 - val_loss: 9.9188\n",
            "Epoch 311/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8586 - val_loss: 8.8601\n",
            "Epoch 312/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9340 - val_loss: 9.4220\n",
            "Epoch 313/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8842 - val_loss: 9.7995\n",
            "Epoch 314/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9756 - val_loss: 8.8628\n",
            "Epoch 315/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9298 - val_loss: 9.9714\n",
            "Epoch 316/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9593 - val_loss: 9.0382\n",
            "Epoch 317/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9287 - val_loss: 10.7220\n",
            "Epoch 318/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1257 - val_loss: 9.2509\n",
            "Epoch 319/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9618 - val_loss: 9.0560\n",
            "Epoch 320/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9223 - val_loss: 9.8532\n",
            "Epoch 321/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1789 - val_loss: 9.3695\n",
            "Epoch 322/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9042 - val_loss: 9.2274\n",
            "Epoch 323/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0850 - val_loss: 8.5627\n",
            "Epoch 324/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1014 - val_loss: 8.8379\n",
            "Epoch 325/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0932 - val_loss: 8.2764\n",
            "Epoch 326/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9922 - val_loss: 10.1536\n",
            "Epoch 327/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8609 - val_loss: 9.4760\n",
            "Epoch 328/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8696 - val_loss: 8.5996\n",
            "Epoch 329/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8364 - val_loss: 8.6564\n",
            "Epoch 330/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9258 - val_loss: 8.1888\n",
            "Epoch 331/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9016 - val_loss: 11.0898\n",
            "Epoch 332/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8220 - val_loss: 8.7528\n",
            "Epoch 333/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8912 - val_loss: 9.5014\n",
            "Epoch 334/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8725 - val_loss: 8.2537\n",
            "Epoch 335/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9117 - val_loss: 9.3367\n",
            "Epoch 336/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.1076 - val_loss: 10.4015\n",
            "Epoch 337/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8734 - val_loss: 8.1705\n",
            "Epoch 338/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8885 - val_loss: 8.7532\n",
            "Epoch 339/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7267 - val_loss: 10.1872\n",
            "Epoch 340/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8334 - val_loss: 9.0146\n",
            "Epoch 341/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8810 - val_loss: 8.8698\n",
            "Epoch 342/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9336 - val_loss: 9.3020\n",
            "Epoch 343/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0277 - val_loss: 11.7052\n",
            "Epoch 344/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 1.0108 - val_loss: 9.3600\n",
            "Epoch 345/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8025 - val_loss: 9.3647\n",
            "Epoch 346/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8353 - val_loss: 10.4733\n",
            "Epoch 347/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7419 - val_loss: 9.7706\n",
            "Epoch 348/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8732 - val_loss: 9.0349\n",
            "Epoch 349/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7941 - val_loss: 10.1530\n",
            "Epoch 350/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7786 - val_loss: 8.7493\n",
            "Epoch 351/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8669 - val_loss: 8.5198\n",
            "Epoch 352/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9340 - val_loss: 10.3927\n",
            "Epoch 353/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8355 - val_loss: 8.5968\n",
            "Epoch 354/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8234 - val_loss: 9.0617\n",
            "Epoch 355/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7839 - val_loss: 7.7004\n",
            "Epoch 356/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9456 - val_loss: 9.7627\n",
            "Epoch 357/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8127 - val_loss: 10.8465\n",
            "Epoch 358/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7772 - val_loss: 9.1924\n",
            "Epoch 359/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7914 - val_loss: 9.2818\n",
            "Epoch 360/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6814 - val_loss: 9.1187\n",
            "Epoch 361/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7937 - val_loss: 9.4074\n",
            "Epoch 362/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7055 - val_loss: 10.1252\n",
            "Epoch 363/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7073 - val_loss: 9.1536\n",
            "Epoch 364/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7856 - val_loss: 9.3977\n",
            "Epoch 365/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7637 - val_loss: 9.4102\n",
            "Epoch 366/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7523 - val_loss: 9.0597\n",
            "Epoch 367/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8331 - val_loss: 9.2900\n",
            "Epoch 368/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9005 - val_loss: 9.0898\n",
            "Epoch 369/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6615 - val_loss: 9.8371\n",
            "Epoch 370/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8563 - val_loss: 9.9321\n",
            "Epoch 371/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7322 - val_loss: 9.6991\n",
            "Epoch 372/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6707 - val_loss: 9.4381\n",
            "Epoch 373/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6530 - val_loss: 8.8162\n",
            "Epoch 374/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7047 - val_loss: 8.9099\n",
            "Epoch 375/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6881 - val_loss: 9.9124\n",
            "Epoch 376/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8999 - val_loss: 9.2505\n",
            "Epoch 377/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8218 - val_loss: 10.3568\n",
            "Epoch 378/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7186 - val_loss: 8.8517\n",
            "Epoch 379/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7542 - val_loss: 9.9970\n",
            "Epoch 380/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.9138 - val_loss: 10.4705\n",
            "Epoch 381/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.8798 - val_loss: 10.0388\n",
            "Epoch 382/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7286 - val_loss: 9.8321\n",
            "Epoch 383/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6527 - val_loss: 8.7221\n",
            "Epoch 384/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6730 - val_loss: 10.7216\n",
            "Epoch 385/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6859 - val_loss: 8.9241\n",
            "Epoch 386/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6843 - val_loss: 9.9696\n",
            "Epoch 387/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7227 - val_loss: 9.5551\n",
            "Epoch 388/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6733 - val_loss: 8.7891\n",
            "Epoch 389/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7078 - val_loss: 10.6367\n",
            "Epoch 390/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6503 - val_loss: 8.8965\n",
            "Epoch 391/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7348 - val_loss: 9.4915\n",
            "Epoch 392/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7347 - val_loss: 8.8662\n",
            "Epoch 393/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6577 - val_loss: 9.1761\n",
            "Epoch 394/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.5708 - val_loss: 9.3921\n",
            "Epoch 395/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6314 - val_loss: 9.4391\n",
            "Epoch 396/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.5988 - val_loss: 9.7924\n",
            "Epoch 397/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6773 - val_loss: 9.8819\n",
            "Epoch 398/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.7117 - val_loss: 9.6132\n",
            "Epoch 399/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6228 - val_loss: 9.5353\n",
            "Epoch 400/400\n",
            "66/66 [==============================] - 0s 5ms/step - loss: 0.6217 - val_loss: 9.0970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib.psplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(h.history['loss'], label='loss')\n",
        "plt.plot(h.history['val_loss'], label='val_loss')\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "ax2 = plt.gca().twinx()\n",
        "#ax2.plot(h.history['lr'], color='r')\n",
        "ax2.set_ylabel('lr',color='r')\n",
        "\n",
        "_ = plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "nxehiTV1-EKV",
        "outputId": "4e040312-241d-45db-e380-537aeb1b1087"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(y_train, model.predict(X_train))\n",
        "plt.scatter(y_val, model.predict(X_val))\n",
        "plt.scatter(y_test, model.predict(X_test))\n",
        "plt.plot([0,30],[0,30]) # Y = PredY line\n",
        "plt.xlabel('true value')\n",
        "plt.ylabel('predicted value')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "brrakA4F_AaE",
        "outputId": "2bf1d88e-f7a2-45ef-8a34-f627ffe20271"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58/58 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'predicted value')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_Y_df=model.predict(X_test)\n",
        "Y_df_true=y_test\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "mse= mean_squared_error(Y_df_true, pred_Y_df)\n",
        "rmse=np.sqrt(mse)\n",
        "r2=r2_score(Y_df_true, pred_Y_df)\n",
        "print('MSE : {: 0.3f}, || RMSE : {: 0.3f}, || R2 : {: 0.3f}'.format(mse,rmse,r2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri3RBlNNCblt",
        "outputId": "567a3b3f-effa-46e7-e102-cc366d8b4e88"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n",
            "MSE :  10.622, || RMSE :  3.259, || R2 :  0.735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The end of program "
      ],
      "metadata": {
        "id": "PtV_Gyh1BdrQ"
      }
    }
  ]
}